# Production Docker Compose Configuration
# Optimized for High-Throughput Document Processing
# Target: 50-100 documents/minute processing capacity (10-20 MB files)

services:
  # OCR Service - PRIMARY BOTTLENECK
  # Throughput Strategy: Multiple replicas with high concurrency
  # OCR is CPU-intensive and takes 20-45 seconds per document (scales with file size)
  ocr-service:
    build: ./ocr-service
    ports:
      - "8000:8000"
    environment:
      - LOG_LEVEL=INFO
      - OCR_DPI=300
      - MAX_FILE_SIZE_MB=20  # Production limit
    volumes:
      - ./ocr-service/src:/app/src:ro
      - ./ocr-service/tests:/app/tests:ro
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - backend-network
    deploy:
      replicas: 3  # Increased from 1 for higher throughput
      resources:
        limits:
          cpus: '2.0'  # OCR needs significant CPU
          memory: 4G   # Tesseract models require memory
        reservations:
          cpus: '1.0'
          memory: 2G

  # NER Service - SECONDARY BOTTLENECK
  # Throughput Strategy: Moderate replicas, balanced concurrency
  # NER is faster than OCR but still CPU-intensive
  ner-service:
    build: ./ner-service
    ports:
      - "8001:8001"
    environment:
      - LOG_LEVEL=INFO
      - SPACY_MODEL=en_core_web_sm
      - MAX_TEXT_LENGTH=1000000
    volumes:
      - ./ner-service/src:/app/src:ro
      - ./ner-service/tests:/app/tests:ro
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - backend-network
    deploy:
      replicas: 2  # Increased from 1 for higher throughput
      resources:
        limits:
          cpus: '1.5'
          memory: 3G   # spaCy models need memory
        reservations:
          cpus: '0.5'
          memory: 1G

  # Embedding Service - MEMORY CONSTRAINED
  # Throughput Strategy: Moderate replicas, lower concurrency per worker
  # Embedding generation is memory-intensive due to transformer models
  embedding-service:
    build: ./embedding-service
    ports:
      - "8002:8002"
    environment:
      - EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
      - LOG_LEVEL=INFO
    volumes:
      - ./embedding-service/src:/app/src
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8002/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - backend-network
    deploy:
      replicas: 2  # Increased from 1 for higher throughput
      resources:
        limits:
          cpus: '1.0'
          memory: 4G   # Transformer models are memory-hungry
        reservations:
          cpus: '0.5'
          memory: 2G

  # Query Service - LOW LATENCY REQUIREMENTS
  # Throughput Strategy: Multiple replicas for concurrent queries
  # Query service handles user requests and needs low latency
  query-service:
    build: ./query-service
    ports:
      - "8004:8004"
    environment:
      - DATABASE_URL=postgresql://postgres:password@postgres:5432/ingestion_db
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      - QDRANT_COLLECTION_NAME=embeddings
      - EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
      - LLM_PROVIDER=mistral
      - MISTRAL_API_KEY=${MISTRAL_API_KEY}
      - MISTRAL_MODEL=mistral-large-2411
      - LOG_LEVEL=INFO
    depends_on:
      - postgres
      - qdrant
    volumes:
      - ./query-service/src:/app/src:ro
      - ./query-service/tests:/app/tests:ro
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8004/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - backend-network
    deploy:
      replicas: 2  # Multiple replicas for query load balancing
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G

  # API Gateway Service - HIGH AVAILABILITY
  # Throughput Strategy: Multiple replicas with load balancing
  # Gateway handles all incoming requests and needs high availability
  api-gateway-service:
    build: ./gateway
    ports:
      - "8005:8005"
    environment:
      - LOG_LEVEL=INFO
      - JWT_SECRET_KEY=${JWT_SECRET_KEY}  # Use environment variable in production
      - JWT_ALGORITHM=HS256
      - JWT_ACCESS_TOKEN_EXPIRE_MINUTES=30
      - DATABASE_URL=postgresql://postgres:password@postgres:5432/ingestion_db
      - REDIS_URL=redis://redis:6379/1
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - REDIS_DB=1
      - RATE_LIMIT_ENABLED=true
      - RATE_LIMIT_REQUESTS_PER_MINUTE=120  # Increased for production
      - RATE_LIMIT_REQUESTS_PER_HOUR=2000   # Increased for production
      - RATE_LIMIT_REQUESTS_PER_DAY=20000   # Increased for production
      - INGESTION_SERVICE_URL=http://ingestion-service:8003
      - OCR_SERVICE_URL=http://ocr-service:8000
      - NER_SERVICE_URL=http://ner-service:8001
      - EMBEDDING_SERVICE_URL=http://embedding-service:8002
      - QUERY_SERVICE_URL=http://query-service:8004
      - ENABLE_CORS=true
      - CORS_ORIGINS=["*"]
      - ALLOWED_HOSTS=["localhost", "127.0.0.1", "0.0.0.0"]
    volumes:
      - ./gateway/src:/app/src:ro
      - ./gateway/tests:/app/tests:ro
    depends_on:
      - redis
      - postgres
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8005/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - backend-network
    deploy:
      replicas: 2  # High availability for gateway
      resources:
        limits:
          cpus: '0.5'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 512M

  # Qdrant Vector Database - OPTIMIZED FOR PERFORMANCE
  # Throughput Strategy: Optimized configuration for high-volume vector operations
  qdrant:
    image: qdrant/qdrant:v1.8.4
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - qdrant_storage:/qdrant/storage
    environment:
      - QDRANT__SERVICE__HTTP_PORT=6333
      - QDRANT__SERVICE__GRPC_PORT=6334
      # Performance optimizations
      - QDRANT__STORAGE__PERFORMANCE__MAX_SEARCH_REQUESTS=100
      - QDRANT__STORAGE__PERFORMANCE__MAX_OPTIMIZATION_THREADS=4
    restart: unless-stopped
    networks:
      - backend-network
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 8G   # Vector operations need significant memory
        reservations:
          cpus: '1.0'
          memory: 4G

  # Ingestion Service - COORDINATION HUB
  # Throughput Strategy: Single replica but optimized for coordination
  ingestion-service:
    build: ./ingestion
    ports:
      - "8003:8003"
    environment:
      - LOG_LEVEL=INFO
      - DATABASE_URL=postgresql://postgres:password@postgres:5432/ingestion_db
      - MINIO_ENDPOINT=minio:9000
      - MINIO_ACCESS_KEY=${MINIO_ACCESS_KEY}  # Use environment variable
      - MINIO_SECRET_KEY=${MINIO_SECRET_KEY}   # Use environment variable
      - MINIO_SECURE=false
      - REDIS_URL=redis://redis:6379
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/0
      - OCR_SERVICE_URL=http://ocr-service:8000
      - NER_SERVICE_URL=http://ner-service:8001
      - EMBEDDING_SERVICE_URL=http://embedding-service:8002
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      - QDRANT_COLLECTION_NAME=embeddings
      - QDRANT_VECTOR_SIZE=384
    volumes:
      - ./ingestion/src:/app/src:ro
      - ./ingestion/tests:/app/tests:ro
    depends_on:
      - postgres
      - minio
      - redis
      - qdrant
      - ocr-service
      - ner-service
      - embedding-service
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8003/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - backend-network
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G

  # PRODUCTION WORKER CONFIGURATION
  # Optimized for 50-100 documents/minute throughput (10-20 MB files)
  
  # OCR Workers - HIGHEST PRIORITY (BOTTLENECK)
  # Throughput Strategy: Multiple replicas with high concurrency
  # Target: 25 concurrent OCR operations (scales with file size)
  celery-worker-ocr:
    build: ./ingestion
    command: celery -A src.celery_app worker --loglevel=info --concurrency=5 --queues=ocr_queue --hostname=ocr-worker@%h
    environment:
      - LOG_LEVEL=INFO
      - DATABASE_URL=postgresql://postgres:password@postgres:5432/ingestion_db
      - MINIO_ENDPOINT=minio:9000
      - MINIO_ACCESS_KEY=${MINIO_ACCESS_KEY}
      - MINIO_SECRET_KEY=${MINIO_SECRET_KEY}
      - MINIO_SECURE=false
      - REDIS_URL=redis://redis:6379
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/0
      - OCR_SERVICE_URL=http://ocr-service:8000
      - NER_SERVICE_URL=http://ner-service:8001
      - EMBEDDING_SERVICE_URL=http://embedding-service:8002
    volumes:
      - ./ingestion/src:/app/src:ro
    depends_on:
      - postgres
      - minio
      - redis
      - ocr-service
    restart: unless-stopped
    networks:
      - backend-network
    deploy:
      replicas: 5  # Increased from 2 for higher throughput
      resources:
        limits:
          cpus: '2.0'  # OCR is CPU-intensive
          memory: 4G   # Tesseract models need memory
        reservations:
          cpus: '1.0'
          memory: 2G

  # NER Workers - MEDIUM PRIORITY
  # Throughput Strategy: Moderate replicas, balanced concurrency
  # Target: 9 concurrent NER operations
  celery-worker-ner:
    build: ./ingestion
    command: celery -A src.celery_app worker --loglevel=info --concurrency=3 --queues=ner_queue --hostname=ner-worker@%h
    environment:
      - LOG_LEVEL=INFO
      - DATABASE_URL=postgresql://postgres:password@postgres:5432/ingestion_db
      - MINIO_ENDPOINT=minio:9000
      - MINIO_ACCESS_KEY=${MINIO_ACCESS_KEY}
      - MINIO_SECRET_KEY=${MINIO_SECRET_KEY}
      - MINIO_SECURE=false
      - REDIS_URL=redis://redis:6379
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/0
      - OCR_SERVICE_URL=http://ocr-service:8000
      - NER_SERVICE_URL=http://ner-service:8001
      - EMBEDDING_SERVICE_URL=http://embedding-service:8002
    volumes:
      - ./ingestion/src:/app/src:ro
    depends_on:
      - postgres
      - minio
      - redis
      - ner-service
    restart: unless-stopped
    networks:
      - backend-network
    deploy:
      replicas: 3  # Increased from 1 for higher throughput
      resources:
        limits:
          cpus: '1.5'
          memory: 3G   # spaCy models need memory
        reservations:
          cpus: '0.5'
          memory: 1G

  # Embedding Workers - MEMORY CONSTRAINED
  # Throughput Strategy: Moderate replicas, lower concurrency per worker
  # Target: 6 concurrent embedding operations
  celery-worker-embedding:
    build: ./ingestion
    command: celery -A src.celery_app worker --loglevel=info --concurrency=2 --queues=embedding_queue --hostname=embedding-worker@%h
    environment:
      - LOG_LEVEL=INFO
      - DATABASE_URL=postgresql://postgres:password@postgres:5432/ingestion_db
      - MINIO_ENDPOINT=minio:9000
      - MINIO_ACCESS_KEY=${MINIO_ACCESS_KEY}
      - MINIO_SECRET_KEY=${MINIO_SECRET_KEY}
      - MINIO_SECURE=false
      - REDIS_URL=redis://redis:6379
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/0
      - OCR_SERVICE_URL=http://ocr-service:8000
      - NER_SERVICE_URL=http://ner-service:8001
      - EMBEDDING_SERVICE_URL=http://embedding-service:8002
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      - QDRANT_COLLECTION_NAME=embeddings
      - QDRANT_VECTOR_SIZE=384
    volumes:
      - ./ingestion/src:/app/src:ro
    depends_on:
      - postgres
      - minio
      - redis
      - qdrant
      - embedding-service
    restart: unless-stopped
    networks:
      - backend-network
    deploy:
      replicas: 3  # Increased from 1 for higher throughput
      resources:
        limits:
          cpus: '1.0'
          memory: 4G   # Transformer models are memory-hungry
        reservations:
          cpus: '0.5'
          memory: 2G

  # Completion Workers - LIGHTWEIGHT COORDINATION
  # Throughput Strategy: Single replica sufficient for coordination tasks
  celery-worker-completion:
    build: ./ingestion
    command: celery -A src.celery_app worker --loglevel=info --concurrency=1 --queues=completion_queue,dead_letter_queue --hostname=completion-worker@%h
    environment:
      - LOG_LEVEL=INFO
      - DATABASE_URL=postgresql://postgres:password@postgres:5432/ingestion_db
      - MINIO_ENDPOINT=minio:9000
      - MINIO_ACCESS_KEY=${MINIO_ACCESS_KEY}
      - MINIO_SECRET_KEY=${MINIO_SECRET_KEY}
      - MINIO_SECURE=false
      - REDIS_URL=redis://redis:6379
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/0
      - OCR_SERVICE_URL=http://ocr-service:8000
      - NER_SERVICE_URL=http://ner-service:8001
      - EMBEDDING_SERVICE_URL=http://embedding-service:8002
    volumes:
      - ./ingestion/src:/app/src:ro
    depends_on:
      - postgres
      - minio
      - redis
    restart: unless-stopped
    networks:
      - backend-network
    deploy:
      replicas: 1  # Lightweight tasks - single worker sufficient
      resources:
        limits:
          cpus: '0.5'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 512M

  # Flower - MONITORING AND OBSERVABILITY
  # Throughput Strategy: Single replica for monitoring
  flower:
    build: ./ingestion
    command: celery -A src.celery_app flower --port=5555 --basic_auth=${FLOWER_USER}:${FLOWER_PASSWORD}
    ports:
      - "5555:5555"
    environment:
      - REDIS_URL=redis://redis:6379
      - CELERY_BROKER_URL=redis://redis:6379/0
      - CELERY_RESULT_BACKEND=redis://redis:6379/0
    depends_on:
      - redis
    restart: unless-stopped
    networks:
      - backend-network
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 512M

  # PostgreSQL Database - OPTIMIZED FOR HIGH THROUGHPUT
  # Throughput Strategy: Optimized configuration for high-volume operations
  postgres:
    image: postgres:15-alpine
    environment:
      - POSTGRES_DB=ingestion_db
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}  # Use environment variable
      # Performance optimizations
      - POSTGRES_INITDB_ARGS=--auth-host=scram-sha-256
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      # Add custom postgresql.conf for production optimization
      - ./postgres/postgresql.conf:/etc/postgresql/postgresql.conf:ro
    restart: unless-stopped
    networks:
      - backend-network
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 8G   # Database needs significant memory for caching
        reservations:
          cpus: '1.0'
          memory: 4G
    command: >
      postgres
      -c shared_buffers=2GB
      -c effective_cache_size=6GB
      -c maintenance_work_mem=512MB
      -c checkpoint_completion_target=0.9
      -c wal_buffers=16MB
      -c default_statistics_target=100
      -c random_page_cost=1.1
      -c effective_io_concurrency=200

  # MinIO Object Storage - OPTIMIZED FOR HIGH THROUGHPUT
  # Throughput Strategy: Optimized configuration for high-volume file operations
  minio:
    image: minio/minio:latest
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      - MINIO_ROOT_USER=${MINIO_ACCESS_KEY}
      - MINIO_ROOT_PASSWORD=${MINIO_SECRET_KEY}
    volumes:
      - minio_data:/data
    command: server /data --console-address ":9001"
    restart: unless-stopped
    networks:
      - backend-network
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 4G   # Object storage needs memory for caching
        reservations:
          cpus: '0.5'
          memory: 2G

  # Redis Cache and Message Broker - OPTIMIZED FOR HIGH THROUGHPUT
  # Throughput Strategy: Optimized configuration for high-volume message processing
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    restart: unless-stopped
    command: >
      redis-server
      --appendonly yes
      --maxmemory 2gb
      --maxmemory-policy allkeys-lru
      --tcp-keepalive 60
      --timeout 300
    networks:
      - backend-network
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G   # Redis needs memory for caching and queues
        reservations:
          cpus: '0.5'
          memory: 1G

  # Prometheus - MONITORING AND METRICS
  # Throughput Strategy: Optimized for high-volume metrics collection
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=30d'  # Increased retention for production
      - '--web.enable-lifecycle'
      - '--storage.tsdb.max-block-duration=2h'  # Optimize for high volume
    restart: unless-stopped
    networks:
      - backend-network
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 4G   # Metrics storage needs memory
        reservations:
          cpus: '0.5'
          memory: 2G

  # Grafana - DASHBOARDS AND VISUALIZATION
  # Throughput Strategy: Optimized for high-volume monitoring
  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD}  # Use environment variable
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_INSTALL_PLUGINS=grafana-piechart-panel
    volumes:
      - grafana_data:/var/lib/grafana
      - ../grafana-dashboards:/etc/grafana/provisioning
    restart: unless-stopped
    networks:
      - backend-network
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 2G
        reservations:
          cpus: '0.25'
          memory: 1G

volumes:
  postgres_data:
  minio_data:
  redis_data:
  qdrant_storage:
  prometheus_data:
  grafana_data:

networks:
  backend-network:
    driver: bridge

# PRODUCTION THROUGHPUT SUMMARY (10-20 MB Files):
# ================================================
# 
# File Size Impact:
# - Minimum file size: 10 MB per document (production requirement)
# - Maximum file size: 20 MB per document (system limit)
# - Average file size: 15 MB per document (midpoint for calculations)
# - Processing time scales with file size: Larger files take proportionally longer
# 
# Expected Performance:
# - OCR Workers: 5 replicas × 5 concurrency = 25 concurrent operations
# - NER Workers: 3 replicas × 3 concurrency = 9 concurrent operations  
# - Embedding Workers: 3 replicas × 2 concurrency = 6 concurrent operations
# - Total OCR Capacity: ~65-150 documents/minute (assuming 20-45s per 15MB doc)
# - Total NER Capacity: ~135-360 documents/minute (assuming 3-8s per doc)
# - Total Embedding Capacity: ~30-72 documents/minute (assuming 5-12s per doc)
# 
# System Bottleneck: OCR processing (scales with file size)
# Realistic Throughput: 50-100 documents/minute (10-20 MB files)
# 
# Resource Requirements:
# - Total CPU: ~25-30 cores
# - Total Memory: ~80-100 GB
# - Storage Growth: ~2.2 TB/day (at 100 docs/min × 20 MB avg)
# 
# Scaling Strategy:
# - OCR Workers: Scale first (primary bottleneck, scales with file size)
# - NER Workers: Scale second (secondary bottleneck)
# - Embedding Workers: Scale based on memory availability
# - Database: Consider read replicas for high query volume
# - Redis: Consider clustering for high message volume
# 
# Note: All calculations assume 10-20 MB file sizes as per production requirements
